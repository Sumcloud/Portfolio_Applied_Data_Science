# Portfolio Minor Applied Data Sciences  
**Firstname** : JuliÃ«n  
**Lastname** : van der Niet  
**Student number**: 18069681  
**Date**: 27 December 2021  
**Project group** : IMPutation  

---
# Table of contents  

- [Portfolio Minor Applied Data Sciences](#portfolio-minor-applied-data-sciences)
- [Table of contents](#table-of-contents)
- [Mandatory requirements](#mandatory-requirements)
  - [Datacamp certificates](#datacamp-certificates)
  - [Reflection on group project contributions](#reflection-on-group-project-contributions)
    - [Task I worked on:](#task-i-worked-on)
    - [Personal STARR-based reflection on writing the paper   <br></br>](#personal-starr-based-reflection-on-writing-the-paper---br)
  - [Reflection on own learning objectives](#reflection-on-own-learning-objectives)
    - [STARR-based reflection on learning objectives](#starr-based-reflection-on-learning-objectives)
  - [Evaluation of the group project](#evaluation-of-the-group-project)
    - [STARR-based reflection on taking more of a leadership role in the last weeks<br></br>](#starr-based-reflection-on-taking-more-of-a-leadership-role-in-the-last-weeksbr)
- [**Subject #1:** Research project](#subject-1-research-project)
  - [Conclusion of our research](#conclusion-of-our-research)
  - [Further research](#further-research)
  - [Visualizations](#visualizations)
  - [Planning](#planning)
  - [Miscellaneous](#miscellaneous)
- [**Subject #2:** Domain knowledge](#subject-2-domain-knowledge)
  - [Literature research](#literature-research)
    - [Miscellaneous literature](#miscellaneous-literature)
  - [Terminology](#terminology)

# Mandatory requirements  

This chapter will cover the obligatory criteria as stated in the evaluation rubric. The evidence to back up claims made in this portfolio will come in the form of hyperlinks or images of evidence material

## Datacamp certificates  
1. [**Introduction to Python**](https://www.datacamp.com/statement-of-accomplishment/course/0d41e08e0f2b3cc9237a598a8ac822b7c8b05860) 
2. [**Intermediate Python**](https://www.datacamp.com/statement-of-accomplishment/course/4d486123dc053ca2694097de7e6127f81316a876)
3. [**Python Data Science Toolbox (Part 1)**](https://www.datacamp.com/statement-of-accomplishment/course/0651e2ac7e9182f90b8a11e40643bea15787c5c1)
4. [**Python Data Science Toolbox (Part 2)**](https://www.datacamp.com/statement-of-accomplishment/course/4282ff9502056eae87859c5cdfe1e966240187b1)
5. [**Statistical Thinking in Python (Part 1)**](https://www.datacamp.com/statement-of-accomplishment/course/62a89048462af20d499a352afac77a6baf1b42df)
6. [**Statistical Thinking in Python (Part 2)**](https://www.datacamp.com/statement-of-accomplishment/course/d05439b77c7d468bc9a735e6db71b967b174e7db)
7. [**Supervised Learning with scikit-learn**](https://www.datacamp.com/statement-of-accomplishment/course/ef474d922c1b699daac6676f33c3dac3a044b759)
8. [**Linear Classiers in Python**](https://www.datacamp.com/statement-of-accomplishment/course/ce7ee8ac0416e06da1dc158a3e022de9d89954d5)
9. [**Introduction to Data Visualization with Matplotlib**](https://www.datacamp.com/statement-of-accomplishment/course/180fd9544eaea8b77a08b47cf27c9f6a9851d6d3)
10. [**Model Validation in Python**](https://www.datacamp.com/statement-of-accomplishment/course/d61e1e1df9b4d12b30c000d77b671337231b72ae)
11. [**Cleaning Data in Python**](https://www.datacamp.com/statement-of-accomplishment/course/d33d4326e44e7ebf3cc2030ac866dbd3a4167c34)
12. [**Data Manipulation with pandas**](https://www.datacamp.com/statement-of-accomplishment/course/4b4119271c4d139e096e13c65fa9ed8364185db5)
13. [**Exploratory Data Analysis in Python**](https://www.datacamp.com/statement-of-accomplishment/course/eba7ce278a04f06ff54a12d224ec07da44e57c74)
14. [**Manipulating Time Series Data in Python**](https://www.datacamp.com/statement-of-accomplishment/course/4408c464b2f72c80b251a7dd9283d78904d9365a)
15. [**Time Series Analysis in Python**](https://www.datacamp.com/statement-of-accomplishment/course/b2116ff973d5f0d97520a8415189bca1db44879a)
16. [**Joining Data with pandas**](/Datacamp%20Certificates%20Backup/Progress_Joining_Data_with_pandas.png)
17. [**Machine Learning for Time Series Data in Python**](/Datacamp%20Certificates%20Backup/Machine%20Learning%20for%20Time%20Sereis%20Data%20in%20Python.png)  

[*Back up of certificates*](/Datacamp%20Certificates%20Backup/)

[*Back to table of contents*](#table-of-contents)<br></br>

---

## Reflection on group project contributions

I didnt't have a specific role during this project, I think I did a bit of everything. However, most of my time has been spent has been spent on both research and the writing of the research paper. I didn't do much on the pipeline but I did work on adding imputation methods found in previous literature and visualizations to visualize method performance.  

The final research paper was my biggest contribution as 90% of it was written by me alone of course with feedback from both group members and teachers. The research included in the paper is also done by me to put our findings in context. The only pieces I didn't write was Hot Deck explanation and I only gave feedback an added to the RNN part.
  

### Task I worked on:  

---  
**Research:**  

- [Research proposal](/Research%20Project/Research%20Proposal/Research_proposal_Applied_Data_Science_project_IMP.pdf)
- [Final Word version research paper in PDF](/Research%20Project/Paper/Final%20Version%20PDF.pdf)
- [Final LaTeX version research paper in PDF](/Research%20Project/Paper/IMP%20Final%20Research%20Paper%20LaTeX%20version.pdf)
- [Research done](Research%20Project/Paper/Some_studies_found_during_research.pdf)
- Some of the previous versions of the research paper. (Some where in the form of live documents and were not preserved)
  - [Penultimate version in 1-9-2022](Research%20Project/Paper/Penultimate%20version.pdf)
  - [Version 1-7-2022](/Research%20Project/Paper/Version%201-7.pdf)
  - [Version 1-5-2022](/Research%20Project/Paper/Version%201-5-2022.pdf)
  - [Version 1-3-2022](/Research%20Project/Paper/Version%201-7.pdf)
  - [Version 28-12-2021](/Research%20Project/Paper/Version%2028-12-2021%201400%20JUL.pdf)
  - [Version 13-12-2021](/Research%20Project/Paper/Version-13-12-2021.pdf)
  
**Communication:**  

- [Internal presentation November 8](Presentations/Internal_November_8.pdf)
- [Internal presentation December 6](Presentations/Internal_Week_6.pdf)
- [Internal presentation December 20](Presentations/Internal_December_20.pdf)
- [Preperation External Presentations](https://www.canva.com/design/DAEvc6PSrRc/QSvuCi_b7rpERo3q_xepXA/view?utm_content=DAEvc6PSrRc&utm_campaign=designshare&utm_medium=link&utm_source=sharebutton)

**Applying research/imputation methods:**  

- [Best correlator picker in Dataframe](/Project%20Notebooks/Best_Correlator_Picker.ipynb)
- [KNN distance weighted regression/algorithm](/Project%20Notebooks/KNN)
  - [Example](/Project%20Notebooks/KNN/KNN_K=5.ipynb)
- [Last Observation Carried Forward (LOCF) or forward filling](Project%20Notebooks/forward_fill.ipynb)
- [Time optimized interpolation](Project%20Notebooks/time_interpolation.ipynb)
- [Linear interpolation](Project%20Notebooks/linear_interpolation.ipynb)
- [Bayesian ridge MICE](Project%20Notebooks/bayesian_ridge_MICE.ipynb)
- [ExtraTreesRegressor MICE](Project%20Notebooks/missForest_regressor_MICE.ipynb)
- [Imputation by mean](Project%20Notebooks/mean_imputation.ipynb)
- [Imputation by median](Project%20Notebooks/median_imputation.ipynb)
- [Imputation by mode](Project%20Notebooks/mode_imputation.ipynb)

**Visualizations:**

- [Difference between original and imputed over gap](Project%20Notebooks/Visuals/Trends_diff.ipynb)
  - [Example 1](Project%20Notebooks/Visuals/Images/Trend_difference_smartMeter_power_Hot_deck_gap_type_5_other_index.png)
  - [Example 2](Project%20Notebooks/Visuals/Images/Trend_difference_smartMeter_power_Hot_deck_gap_type_5.png)
- [Missingno Matrix](/Project%20Notebooks/Visuals/missingno_matrix.ipynb)
  - [Example](/Project%20Notebooks/Visuals/Images/Missingno_matrix_smartMeter_power_gap_type_5.png)
- [Evaluation criteria per gap per method in a grouped bar chart](/Project%20Notebooks/Visuals/criteria_grouped_bar_plotter.py)
  - [RMSE example](/Project%20Notebooks/Visuals/Images/Root%20Mean%20squared%20error_per_gap_co2sensor_co2_6_17_33_41.png)
  - [Variation Error Example](/Project%20Notebooks/Visuals/Images/Absolute%20Variance%20error_per_gap_alklimaHeatPump_flow_temp_6_13_34_58.png)
  - [Percent Bias Example](/Project%20Notebooks/Visuals/Images/Percent%20bias_per_gap_smartMeter_power_6_17_33_56.png)
- [Results loader a script to automate graphs from results](/Project%20Notebooks/Visuals/Results_Auto_Loader.py)
- [Value distribution histogram](/Project%20Notebooks/Visuals/distribution_of_meassurements.ipynb)
  - [Example](/Project%20Notebooks/Visuals/Images/Distribution_in_measurements_power_imputed_by_hot_deck_gap_type_5.png) 
- [Change in Value Distribtuion histogram](/Project%20Notebooks/Visuals/change_in_distribution_histogram.ipynb)
  - [Example](/Project%20Notebooks/Visuals/Images/Change_in_distribution_histogram_hotdeck_gap_size_5.png) <br></br>

### Personal STARR-based reflection on writing the paper   <br></br>

**Situation:**  
In the month of December, it was thought that the Research paper was further along than it was at the time. To finish or get close to finishing the paper a time window of a month and a half was created. In the first week of December, it turned out there was not a lot done yet. Which left three weeks before the Christmas break to get a close to the final version draft done. The research was also still very light to support the paper so that needed some work too
Accompanying research for the paper was also in need of improvement to get a starting point for our paper.

**Task:**  
Write the research paper and improve on it iteratively based on the feedback given by teachers and group members. Whilst writing reinforce the research paper with previous literature on the subject. Whilst writing the paper also look for what visualizations to include for getting the point across better.

**Action:**  
I started writing the paper on the third of December and added the required research from what I had read before and was reading. To get everything correct in terms of technical details for our paper I asked every person for a detailed description of how the technical aspects work. These descriptions were rewritten to be included in the paper. 
To get as much feedback as possible I arranged for extra meetings with teachers and made a live document on google drive for group members to give feedback in real-time. Because of the late start, I worked hard on this until the final weekend of the Christmas break. Every three days a new version was sent for feedback during the vacation.
Whilst researching I also made suggestions as to what figures would help get our point across well and later, I wrote code that gave better insight data for our personal evaluation and for the final paper.

**Result:**  
A formatted research paper was written to communicate the research done during this project. Iteratively written to get as much feedback as possible to make sure there were no misinterpretations. Including 3 or 4 figures that were made by me.

**Reflection:**  
Because of the unexpected late start with writing the paper I had to spend a lot of the Christmas break working on the paper. Due to the writing style and other people being busy or not available during the Christmas vacation I wrote the paper mostly by myself. If I had to do this over again, I would probably have checked on the paper sooner to prevent losing time. With writing the paper I would have liked to be more inclusive by convincing other people to write too. Aside from that, I think I communicated clearly what my progress was and what needed feedback to my group members.


## Reflection on own learning objectives  

---
I wanted to use this minor to get an introduction to data science so that I could get an impression on if it is something that interests me as a career choice. I had no specific learning objectives in mind when starting this semester but during the minor, I developed a couple of learning objectives that I wanted to achieve. These were:

1. Writing and submitting a research paper
2. Competency in Python programming
3. Using visualizations and data to draw and support sound conclusions.

I have written research reports and essays before but never a formatted research paper. During this semester I tried to write a well-written paper that could be accepted at an official conference. I personally enjoy writing and feedback given by teachers and our problem owner has given me a better understanding of writing papers.  

I had used Python before but only as a quick scripting language for formatting documents or quick calculations, but it never got much deeper than this. Through the data camp courses, I got a more in-depth understanding of Python programming in normal Python and for Jupyter Notebook. Using Python to make quick and easy visualizations of data has been something I wanted to do for some time and has been a definite highlight for me in terms of Python competency. 

Aside from writing the paper I also wanted to learn about how to make conclusions based on data and its visualizations for a project like this. Learning this and applying the skill in Python and in writing a paper is going to be an asset for later. <br></br>    

### STARR-based reflection on learning objectives  


**Situation:**  
After the pipeline had finished running the imputations and evaluating the selected methods it created a CSV file filled with performance metrics per imputation performed.  The results need to be properly formatted, visualized and studied to make sound conclusions to our research for the paper. This then needed to be written down in a readable fashion for our paper. For the paper a selection of what visuals to include also needed to be made since there was a limit on how many visuals could be included for the conference the paper was going to be submitted to.

**Task:**  
Get the results from the pipeline create visualizations based on the metrics and raw data also format them to make them readable in text. When that is done analyze and study the results achieved by our selected imputation methods and compare them to get the conclusion for our research paper. 

**Action:**  
When the results were in, I downloaded them from the pipeline (Jupyter Notebook server) and loaded them into a visualization script that I wrote. The script automatically generated visuals for the results on selected criteria in the results.csv file. When the results were visualized, I started analyzing the data visualized and in text form to draw conclusions.  

**Result:**  
A sound conclusion was made on the results gotten from the comparison experiment done during our research. The conclusion in the research paper was properly structured and easy to understand for the reader. Visualizations selected for the conclusions also give a visual insight into the performance of the selected imputation methods that further support our conclusion.

**Reflection:**  
Due to the Christmas break, it was hard to get all the group members into one call to give a consensus for our conclusion. To make it work I tried to be as transparent as possible on how the conclusion was made using visuals and data points. I think I succeeded in giving people clear insight into how and why the conclusions were made. 

Thinking back, I would have liked more meetings with the whole group at once to discuss the results in one go. Instead, I made sure to hear everyoneâs opinion in some way or form in a different call or in text format. I would have liked to make more visualizations for the paper but due to restrictions in formatting that wasnât allowed.   <br></br>


## Evaluation of the group project    

--- 

Our group had people of various nationalities, which meant we had to communicate in English instead of Dutch. This was my first English project although I have written English reports before. All group memberâs English-speaking abilities were great, but it still felt hard at times to communicate specific parts of the project. Sometimes it felt in speaking and hearing that the semantics of what some was trying to say fell off which created some small inconsistencies in collaboration.

I think we all got along during this project and the arguments we did have were solved in a mature manner. 

### STARR-based reflection on taking more of a leadership role in the last weeks<br></br>

**Situation:**  
During the last weeks of the project, we as a project group felt a bit aimless. Some group members werenât sure of what they should do but they also were not asking. We were stuck at the evaluation part of the research which made progressing further harder. Exchange students were also going back to their home countries which made physical meetings with the whole group impossible. The fact that all communication now had to be digital, and the holidays made it harder to keep progress steady. 

**Task:**  
Make sure everyone has a task to do for the research project and what their work entails. Deadlines also need to be made clear for keeping progress as smooth as possible when working individually.

**Action:**  
Because I was writing the paper, I had the best overview of what had to be done still to complete our research. When looking at Jira and asking people what they were working on I got an understanding of who was doing what and was available to complete the tasks to be done.
When working physically together at school I asked people in person at school what they were working on and if they could finish a part still to be completed. I also discussed the things I was missing in the research with the group, so everyone understood who did what. While working online I did mostly the same I messaged everyone individually on what needed to be done and when and in the WhatsApp group I also announced who did what. 
On January 4th we met to finish the paper and for that, the task people were tasked with needed to be done. To make it clearer on what was expected from everyone I created an agenda points document. Jan 4th didnât turn out to be the finishing date but for the other days, I created a similar document or message.

[Example of agenda point document](/Research%20Project/Agenda_points_for_jan_4th.pdf)
<details>
  <summary>Image agenda point document</summary>
  <img src="/Research%20Project/Image_Agenda_Points_Document.png" alt='image agenda point document'>
</details>  

**Result:**  
Clearer coordination and communication within the group everyone knew what they were supposed to do before Jan 4th and other dates. It also helped writing the paper because it made it easier to communicate when a new version of the paper was ready for feedback.

**Reflection:**  
The group should probably have appointed someone as a âleaderâ it felt we were a bit aimless at times. This was also due to some people being very reactive instead of active. Whether it was me or someone else doesnât really matter to me. But I feel that if I started being more assertive earlier on in the project, we could have had a more balanced workload for the pipeline and paper. This would probably have resulted in a more gradual improvement too compared to the explosive bursts we had in progress.   

# **Subject #1:** Research project  

The project I worked on was project-imputation or IMP, the goal of the project was to create a guideline for the imputation of Building Management System (BMS) time-series data. The client for this project was the research group Energy in Transition (EiT), a research group from THUAS. During the project, we had constant contact with our contact person at EiT Mr. Baldiri Salcedo Rahola. EiT provided the research group with the datasets used for the research. The datasets contained data from 120 BMS units placed in terraced houses collected over the year 2019. Aside from BMS data our research also used meteorological data from KNMI. This data set contained climate recordings from 2018-2021 and included measurements taken by 25 weather stations throughout the Netherlands.  

BMS measurements in our data set were supposed to be at an interval of five minutes but due to sensor malfunction or data storage errors, these values can be lost. These losses add up and cause inconsistencies down the line in downstream applications of the data. An example of this can include worsened performance in forecasting in cases with enough data missing. 

To get the view of the project group, the teachers, and the client in line a research proposal was created to create an outline for our research. To form our research question and corresponding sub-questions preliminary literature research was done to see what previous work could be built upon.  
The proposal contained our research questions as listed below:

**Main-question:**  
 - Which imputation techniques should be applied for data imputation in building energy time series data?  

**Sub-questions:**  
  1. What imputation methods are known for imputing time-series data?
  2. Which imputation methods are best suited for what gap sizes
  3. What imputation methods are best suited for which types of data?  

**Explanation:**

The first sub-question allowed us to orientate ourselves more in previous literature to find what methods have precedent in the field. The selection of imputation methods that were included was 4 but more methods outside those four were tested internally.   

Due to the nature of some imputation methods (some imputing dynamic or static values) performance on various amounts of missing data and gap sizes is to be expected. During our preliminary desk research that was confirmed which led us to include this as a research question. To mimic gaps found in real data a gap creation algorithm was made to create artificial gaps randomly according to rules set in a config file. This allowed us to test the performance of each method in a controlled environment.   

From preliminary research done in the first weeks of the project, we found that some methods of imputation will most likely perform better on certain data measurements scales. This led us to include it as a research question for our project. The conclusion of research question 3 formed the scenarios for the guideline together with the second question. Our data sets didnât contain all measurement scales because of the nature of the data set ordinal scale data wasnât included. The nominal data also was numerical and not text-based.
The main research question was answered by testing the methods found in sub-question 1 and testing their performance in sub-question 1 and 2. The conclusion to our paper came in the form of a guideline of what imputation method to use with what data measurement scale and gap size.

During our research, we deviated from the research proposal as imputing trends back into missing data became a focal point. This also meant a change in evaluation metrics as RMSE might not evaluate the ability to impute trends well. For this reason, Variance Error or VE was chosen to evaluate our imputation performance. RMSE was still included in the final paper as it is a common evaluation metric in many previous works.

For our paper we used various abbreviations to make things clearer a table explaining them is found below.

| Abbreviation| Term in full |
|----------------:|-----------:|
|RMSE| Root Mean Squared Error|
|VE| Variance Error|
|HD| Hot Deck|
|RNN|Recurrent Neural Network|
|GRU|Gated Recurrent Units|
|KNN|K-Nearest Neighbour|
|LOCF|Last Observation Carried Forward|



## Conclusion of our research  

This paper proposes a guideline to impute BMS nZEB data based on gap size and different scales of measurement.. The problem with missing data in BMS is becoming a bigger problem in an era where buildings depend on data. Previous research has been done about imputing BMS time series data; this paper tries to build on that by creating a comprehensive guideline to follow for certain scenarios. To create a guideline 4 methods were chosen from previous literature: GRU RNN, Hot Deck, KNN algorithm and LOCF. During the research, imputing trends back into missing data became the focal point of this study which is why Variance Error (VE) was used instead of a more traditional metric like Root Mean Squared Error (RMSE). The guideline that resulted from this experiment is listed down in the table below. Performance was evaluated using both RMSE and VE but metrics concluded the same methods as best for each gap type and data measurement scale.

From the results of both VE and RMSE can be concluded that there is no single best imputation method for all gap sizes and measurement scales. The best method for a gap size is dependent on the measurement scale of the to be imputed data. No consistent crossover was found between the gap size and measurement scale as can be seen in the table.   

| Guideline table | Gap type 1 | Gap type 2 | Gap type 3 | Gap type 4 | Gap type 5 | 
|----------|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|
|**Nominal**|HD|HD|HD|HD|HD|
|**Ratio**|HD|HD|HD|HD|HD|
|**Interval**|RNN|RNN|RNN|RNN|RNN|

[**The values this table was based on can be found here**](/Research%20Project/Paper/Results%20Based%20on%20VE%20and%20RMSE.pdf)

Outside the main conclusion several other smaller conclusions can be made based on the imputation results from our experiment. These smaller conclusions are:

- The RMSE results show that the imputations done in this study are poor when compared to previous literature.
- HD tends to do better in the KNMI datasets compared to RNN. This might be due to there being more regularity in data and thus more similar trends. An interesting example of this in the KNMI temperature data where HD beats RNN by a small margin but in flow_temp it loses to RNN with a wide margin in both VE and RMSE. 
  
- RMSE and VE do not always align when it comes to accurately evaluating the ability to impute a trend back into the missing data. A good example of this is CO2 gapsizes 3 and 4, the RMSE is score is relatively close between RNN and HD while the VE score is far apart. In the image below it can be seen that HD is trying to impute a trend and RNN is imputing a more stable is imputed.<details>
  <summary>Co2 gap size 4 visual</summary>
  <img src="/Research%20Project/Paper/CO2%20Sensor%20Gap%204.png" alt="Co2 gap size 4 visual">
</details>  

- In the acheived results no strong link can be found between having multiple strong correlators and a good RMSE or VE score. Flow_temp had two strong correlators in heat pump power usage and return temperature and got good imputation results. Power and CO2 results however seem to contrast these findings, as power and co2 both had one or two decent to strong correlators but both got worse results. 

- In previous research, it was found the distribution to matter when training and imputing data across different units. To measure, the impact of this factor the Kurtosis and Skewness of both the training and imputation target were recorded. From our results, we can conclude that there was no consistent impact of a difference in Kurtosis or Skewness affecting our results. The CO2 sensor data had a high difference in both Kurtosis and Skewness and that might have affected the RNN somewhat. However, when looking at other features like flow_temp or power that same effect doesnât show the same result. The difference in Kurtosis or Skewness canât explain the worse performance of RNN on ratio data.

## Further research

In future work, the focus of research should be less on evaluating imputation with metrics based on the error and more on the impact of forecasting using imputed data. The effect on forecasting performance ought to be evaluated as it can provide a more complete view of imputation performance. 

The data sets used for this study contain only numerical data and no ordinal data. To get a full view of the imputation performance on text-based categorical data further research is required.

The GRU RNN architecture used in the research had clear limitations based on how it was set up. To evaluate the full potential of imputation using RNN the architecture should be changed to an encoder-decoder sequential based design. This would remove the potential bias of imputation using its own imputed values.

To see if Hot Deck is truly a viable method for the imputation of BMS data more research will have to be done when the data sets aren't as similar. Since the buildings in this study are all in the same neighbourhood trends in power usage are very similar.

## Visualizations

To help make conclusions about the data I made visualizations of the results per imputation target and evaluation criteria. These visuals were automatically generated when loading data in a script I wrote. 

- [Link to the script here!](Project%20Notebooks/Visuals/criteria_grouped_bar_plotter.py)  
- <details><summary>Example image</summary><img src="/Project%20Notebooks/Visuals/Images/Average%20variance%20error_per_gap_Temperature_9_20_2_37.png" alt="Variance average temperature" ></details>
- [Other examples](/Research%20Project/Paper/Graph%20examples.pdf)

To get ideas for visualizations to include in the paper I experimented around with some visuals. Sadly because of the CLIMA-format heavily restricting the use of visuals many of these concepts didn't make it into the paper. The visual concepts included:

- Missingno matrix <details><summary>Example</summary><img src="/Project%20Notebooks/Visuals/Images/Missingno_matrix_smartMeter_power_gap_type_5.png" alt="missingnomatrix"></details>
- Change in distribution <details><summary>Example<summary></details>
- Distribution histogram <details><summary>Distribution histogram</summary></details>
- Trend indicator line graph <details><summary>Example</summary><img src="/Project%20Notebooks/Visuals/Images/Trend_difference_smartMeter_power_Hot_deck_gap_type_5_other_index.png" alt="trend difference Hot Deck gap type 5"></details>


## Planning
At the start of the project we made a verbal agreement on what to finish when. Our goal was to present the research phases during the external presentations but we deviated from that when it came into practice. During the first weeks we had meetings once or twice a week but later on we upped that to daily meetings.

For visualizing our KANBAN board we made of use of JiRA. Tasks were added weekly on Thursdays after meeting together and with teachers and at the end of each sprint we added backlog tasks for the next one. We didn't do traditional retrospectives but at the end of each sprint (which was always a Thursday) we discussed what we had done and what needed extra attention for the next sprint.

Some weeks of the project had to be spent online due to COVID restrictions and in those weeks we held online daily standups to remain productive. During this project I had to go in to a quarantine  because a close relative got COVID, this lasted 2+ weeks which made it hard to keep as involved in the project.

<details>
  <summary>Jira Sprint example</summary>
  <img src="Research%20Project/JiRa%20sprints/Sprint_6.png" alt="Sprint 6">

  [Sprint backup image](/Research%20Project/JiRa%20sprints/Sprint_6.png)
</details>  

## Miscellaneous 

During the project I have done research to back up our findings. These studies can be found [here](/Research%20Project/Paper/Some%20of%20the%20studies%20found%20and%20described.pdf) formatted in a table. More papers were read but because only the most relevant papers were kept the list can feel relatively short. 

[Back to table of contents](#table-of-contents)


# **Subject #2:** Domain knowledge

This project focused on imputing Building Management System (BMS) time-series data. BMSâs generate data from various sensor measurements at set time intervals. Examples of these sensors are the current operational mode of a heat pump, heat pump water flow temperature, power usage and CO2 measurements. However, sometimes data can get lost by things like sensor or data storage malfunction. Lost data in large enough quantities cause forecasting models to be less accurate and can thus by proxy result in less living comfort for the residents or worse building system shutdown. 

This is where imputation comes in, to bring back the lost data many methods can be used with various backgrounds e.g., Statistical, Linear or Neural Network-based methods. For smaller gaps simpler methods can be used with relative success but in larger sequences of missing data more complex methods ought to be used. Examples of simpler methods are Last Observation Carried Forward and Linear interpolation. LOCF works by placing the last valid observation on the entirety of the gap and Linear interpolation calculates a linear trend between the value before and after a gap.
More complex methods would include Neural Network or machine learning based solutions. 

Data from things like power usage can be hard to impute because the data changes rapidly and is very volatile. When imputing things like power usage or flow temperature it can be useful to know trends since people have certain habits. The imputation of trends is also important for the same reason as it gives better insight into downstream applications of the data. 

Because our buildings are becoming smarter and smarter and more dependent on data to regulate the efficiency of their processes completeness of data is vital. There has been a lot of research into the imputation of BMS data to make the buildings of the future more efficient. Our research aimed to create a guideline on when to use what imputation method for what measurement scale of data and amount of data missing. 

Another focus of our research paper was to evaluate an imputation methodâs ability to impute a trend back into the missing data. Previous literature used an evaluation metric called Root Mean Squared Error (RMSE) which in different circumstances is a good metric to evaluate imputation performance with. Because RMSE cares more about the total accuracy of imputation than a trend being followed it can give misleading insight into an imputation methodâs ability to impute trends.

For our research, a different metric was chosen namely the Variance Error (VE). It is calculated by calculating the difference in the variance of data in original and imputed data. RMSE was still included in the paper as a comparison point for previous and future research. RMSE tended to match VE but in some cases, RMSE scores indicated a different conclusion than VE but the trend imputation was correctly evaluated by VE. 


## Literature research

To gain knowledge on the specific domain of the imputation of BMS time-series data studies were used that are included in the final paper.
Those can be found [here](Research%20Project/Paper/Some_studies_found_during_research.pdf).

To highlight some of the research found:  

- Liang Zhang (2020), A study into the imputation of BMS time series data using a comparative experiment.This gave me a good idea of the current work done in the domain.
https://doi.org/10.3390/s20205947 

- Mel Keytingan (2020), What methods have precedent in the field and what to take into account when dealing with seperate data units.  
  https://doi.org/10.1016/j.dibe.2020.100037
-  Mehdi Pazhoohesh (2019), Not especially similar data set as it uses lighting data instead of things like power. Still gives good precedent on methods to include when imputing time series.
    https://doi.org/10.1109/sege.2019.8859963

### Miscellaneous literature
- Stef van Buuren's book Flexible Imputation of Missing Data, this was useful when starting out with imputations. https://stefvanbuuren.name/fimd/
- Some articles from [towards data science (medium.com)](https://towardsdatascience.com/) for example this [article](https://towardsdatascience.com/a-comprehensive-guide-to-data-imputation-e82eadc22609). They were quite useful to get some layman's knowledge in the field.


## Terminology 

|Term   |Explanation|
|:------|:----------|
|Imputation|Replacing the missing values with substituted data|
|Variance| mean squared difference between each observation and the centre of distribution meaning the average|
|Kurtosis|Statistical indicator of the peakedness in distribution of data. Data sets with a high Kurtosis value will be heavily tailed and ones with low Kurtosis values will be light tailed in their distribution.|
|Skewness|Meassure of asymmetry in probability distribution about its mean|
|Variance Error| The difference in variance present in original and imputed data|
|Root Mean Squared Error|Common use imputation evaluation metric calculated by taking the square root of the mean squared error|
|Time-series data|A series of observations indexed by a timestamp|
|Data velocity|How quickly data is generated in our case supposedly every 5 minutes|
|Machine learning|Systems that are able to learn without explicit instructions by using algorithms and other statistical values to make conclusion based on patterns observed in data.|
|Neural Network|Algorithms that mimic the process of the human brain to recognize patterns in data sets.|

[Back to the table of contents](#table-of-contents)